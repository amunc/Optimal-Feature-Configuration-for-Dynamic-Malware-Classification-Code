# -*- coding: utf-8 -*-
"""
Created on Mon Jul 27 08:26:42 2020

@author: David
"""

import itertools
from enum import IntEnum, Enum
from .dataframe_filtering.filtering import (
        filter_dataframe
)
import os
from ..utils.settings import load_object, load_object_with_params
import json
from collections import defaultdict, UserDict
import numpy as np
import pandas as pd
import logging
from ..utils.initialization import create_directory
import pathlib
from .support_classes import DescriptiveStats, ResultDict, Order
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)



def get_loader(settings):
    info = settings["STATISTICS_DATA_LOADER"]
    return load_object_with_params(info)


class StatisticsManager:
    def __init__(self, settings):
        self.pvalue = settings["BASE_PVALUE"]
        self.data_columns = settings["STATISTICS_DATA_COLUMNS"]
        self.descriptive_columns = settings.get(
                "STATISTICS_DESCRIPTIVE_COLUMNS", [])
        self.normality_test = load_object(settings["NORMALITY_TEST"])
        self.difference_test = load_object(settings["DIFFERENCE_TEST"])
        self.posthoc_test = load_object(settings["POSTHOC_TEST"])
        self.posthoc_normal_test = load_object(settings["POSTHOC_NORMAL_TEST"])
        self.difference_function = load_object(settings["DIFFERENCE_FUNCTION"])
        self.loader = get_loader(settings)
        self.redundant = settings.get("REDUNDANT_INFORMATION", [])
        self.redundancy_removal_selector = load_object(
                settings["REDUNDANT_INFORMATION_REMOVAL_FUNCTION"])
        self.file = settings["STATISTICS_INPUT_FILE"]
        self.output_file = settings["STATISTICS_OUTPUT_FILE"]
        self.output_path = settings["STATISTICS_OUTPUT_PATH"]
        self.evaluation_order = settings["COLUMN_EVALUATION_ORDER"]
        self.elimination = settings["STATISTICS_BY_ELIMINATION"]
        self.register_intermediate = settings["REGISTER_INTERMEDIATE_RESULTS"]

    def compute_normality_single(self, *values):
        return map(lambda x: self.normality_test(x) >= self.pvalue, values)

    def compute_normality_all(self, *values):
        return all(self.compute_normality_single(*values))

    def remove_redundant_info(self, df, by=None, metrics=None):
        if not by:
            by = set(df.columns) - self.data_columns
        if not metrics:
            metrics = self.data_columns
        to_delete = []
        for redundant_dict in self.redundant:
            redundant_col, redundant_vals = next(iter(redundant_dict.items()))
            if len(redundant_vals) <= 1:
                logger.warning(
                    ("Number of redundant values lower than 2, cannot compute"
                     + ": %s", redundant_vals)
                    )
                continue
            missing_vals = (set(redundant_vals)
                            - set(df[redundant_col].unique()))
            if missing_vals:
                logger.warning(
                    "Redundant values %s not found in column %s",
                        missing_vals, redundant_col)
                continue
            dicts = dict.fromkeys(metrics, {})
            for metric in metrics:
                dicts[metric]["ranking"], dicts[metric]["comparisons"] = (
                        self.redundant_info_comparison_single(df, by, metric,
                                                              redundant_dict)
                )
                if self.register_intermediate:
                    self.write_intermediate(ranking, results, metric,
                                            extra_dir="redundancy")
            to_delete.append(
                    {redundant_col: self.redundancy_removal_selector(
                                           dicts, redundant_vals,
                                           by.index(redundant_col)
                                        )}
            )
        # Delete after processing all redundancies just in case there are
        # overlaps between dicts
        to_delete_dict = defaultdict(list)
        for d in to_delete:
            for key, val in d.items():
                to_delete_dict[key].extend(val)
        return filter_dataframe(df, to_exclude=to_delete_dict)

    def redundant_info_comparison_single(self, df, by, metric, redundant_dict):
        grouped = df.groupby(by)[metric]
        redundant_col, redundant_values = next(iter(redundant_dict.items()))
        valued = filter(
                lambda x: np.isin(x[0], redundant_values).any(), grouped)
        combinations = itertools.combinations(valued, 2)
        index = by.index(redundant_col)

        def compatible(pair):
            first, second = pair
            # Same value in redundant_col
            if first[0][index] == second[0][index]:
                return False
            first_ = [x for i, x in enumerate(first[0]) if i != index]
            second_ = [x for i, x in enumerate(second[0]) if i != index]
            if first_ == second_:
                return True
            return False
        selected_comb = filter(compatible, combinations)
        ranking, results = self.compute_difference(selected_comb)
        return ranking, results

    def compute_difference(self, groups):
        results = ResultDict()
        differences = {}
        i = 0
        for i, ((tup_1, data_1), (tup_2, data_2)) in enumerate(groups):
            if self.compute_normality_all(data_1, data_2):
                pval = self.posthoc_normal_test(data_1, data_2)
            else:
                pval = self.posthoc_test(data_1, data_2)
            if tup_1 not in differences:
                differences[tup_1] = DescriptiveStats(
                        tup_1, value=self.difference_function(data_1))
            if tup_2 not in differences:
                differences[tup_2] = DescriptiveStats(
                        tup_2, value=self.difference_function(data_2))
            results[tup_1, tup_2] = pval
        i += 1
        for key, pvalue in results.items():
            if pvalue < self.pvalue / i:
                lower = differences[key[0]].value < differences[key[1]].value
                differences[key[lower]].count += 1
                results[key] = (pvalue, Order.LOWER if lower else Order.GREATER)
            else:
                results[key] = (pvalue, Order.EQUAL)
        ranking = sorted(differences.values(), reverse=True)
        return ranking, results

    def get_best(self, df, by=None, metrics=None):
        '''
        Computes the value of a certain descriptive column that provides the
        best results.

        Arguments
        ----------------------
        df: pandas.DataFrame
            dataframe that contains the results.
        by: array-like
            list that contains the descriptive columns whose best combination
            of values is going to be computed
        '''
        if not by:
            by = list(set(df.columns) - self.data_columns)
        if not metrics:
            metrics = self.data_columns
        results = defaultdict(dict)
        for metric in metrics:
            results[metric]["ranking"], results[metric]["comparisons"] = (
                    self.compute_metric_best(df, by, metric)
            )
        return results

    def get_best_by_elimination(self, df, by=None, metrics=None):
        if not by:
            by = list(set(df.columns) - self.data_columns)
        if not metrics:
            metrics = self.data_columns
        results = defaultdict(dict)
        for metric in metrics:
            results[metric]["ranking"], results[metric]["comparisons"] =(
                    self.compute_best_metric_by_elimination(df, by, metric)
            )
        return results

    def compute_best_metric_by_elimination(self, df, by, metric):
        evaluated = []
        if not set(self.evaluation_order).issubset(set(by)):
            raise ValueError("Columns in evaluation order are not in descriptive columns")
        to_evaluate = (self.evaluation_order
                       + list(set(by) - set(self.evaluation_order))
        )
        #by[:-1]:  # Only reduce until one dimension is left
        for column in to_evaluate[:-1]:
            evaluated.append(column)
            rest_cols = list(set(by) - set(evaluated))
            filtered_dfs = df.groupby(rest_cols)
            selected_tups = []
            for _, filtered in filtered_dfs:
                ranking, results = self.compute_metric_best(
                    filtered, by, metric)
                selected_tups.append(ranking[0])
                if self.register_intermediate:
                    self.write_intermediate(ranking, results, metric,
                                            rest_cols)
            df = filter_dataframe(df, to_include={tuple(by): selected_tups})
        return self.compute_metric_best(df, by, metric)

    def void_differences(self, groups):
        ranking = []
        for tup, data in groups:
            ranking.append(
                    DescriptiveStats(tup, 0, self.difference_function(data)))
        results = ResultDict(
                map(lambda x: ((x[0][0], x[1][0]), (1, Order.EQUAL)),
                    itertools.combinations(groups, 2))
        )
        ranking = sorted(ranking, reverse=True)
        return ranking, results

    def compute_metric_best(self, df, by, metric):
        grouped = df.groupby(by)[metric]
        if grouped.ngroups >= 3:
            pvalue = self.difference_test(
                    *[g[-1].values for g in grouped])
            if pvalue >= self.pvalue:
                return self.void_differences(grouped)
        return self.compute_difference(itertools.combinations(grouped, 2))

    def adjust_columns(self, df):
        if not self.descriptive_columns:
            self.descriptive_columns = list(
                                           set(df.columns) - self.data_columns)

    def _process(self):
        df = self.loader.load(self.file)
        self.adjust_columns(df)
        df = self.remove_redundant_info(
                df, self.descriptive_columns, self.data_columns)
        if self.elimination:
            return self.get_best_by_elimination(
                    df, by=self.descriptive_columns, metrics=self.data_columns)
        return self.get_best(
                    df, by=self.descriptive_columns, metrics=self.data_columns)

    def write_results(self, results, file):
        def dumper(obj):
            if "to_json" in dir(obj):
                return obj.to_json()
            return json.dumps(obj)
        with open(file, "w") as f:
            json.dump(results, f, indent=4, default=dumper)

    def write_intermediate(self, ranking, results, metric, rest_columns=None,
                           write_columns=False, extra_dir=""):
        if rest_columns:
            unique_indices = [self.descriptive_columns.index(col)
                         for col in rest_columns]
            unique_vals = [ranking[0].name[ind] for ind in unique_indices]
        else:
            unique_vals = ["final"]
        base_path = (pathlib.Path(self.output_path) / extra_dir / metric
                     / '_'.join(unique_vals))
        create_directory(base_path)
        df_ranking = pd.DataFrame(
            [list(tup.name) + [tup.count, tup.value] for tup in ranking],
            columns=self.descriptive_columns + ["Count", "Median_%s" % metric]
        )
        df_ranking.to_csv(str(base_path / "ranking.csv"), header=True,
                          index=False)
        # To load .apply(lambda x: x.strip("()").replace("'", "").split(","))
        df_results = pd.DataFrame(
            [[tup1, tup2, pvalue, order.value]
             for (tup1, tup2), (pvalue, order) in results.items()],
            columns=["First", "Second", "pvalue", "Order"]
        )
        df_results.to_csv(str(base_path / "results.csv"), header=True,
                          index=False)
        if write_columns:
            d = {"all": self.descriptive_columns}
            if rest_columns:
                d["rest"] = rest_columns
            with open(base_path / "columns.csv", "w") as f:
                json.dump(d, f)

    def process(self):
        logger.info("Starting processing")
        results = self._process()
        logger.info("Processing finished, saving results")
        for metric, res in results.items():
            self.write_intermediate(res["ranking"], res["comparisons"],
                                    metric, write_columns=True)
        #self.write_results(results, self.output_file)


def compute_statistics(config):
    sm = StatisticsManager(config)
    sm.process()

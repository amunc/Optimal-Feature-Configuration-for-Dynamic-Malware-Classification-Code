# -*- coding: utf-8 -*-
"""
Created on Mon Jul 13 10:40:08 2020

@author: David
"""
from functools import reduce
import itertools
import operator
import numpy as np
import pandas as pd
from typing import Union, List, Iterable, Any, Sequence, Optional, Mapping
from ...utils.general import is_iterable


def mask_single(df: pd.DataFrame, column: Union[Any, Sequence[Any, ]],
                values: Union[Iterable[Any], Any],
                exclude: bool = False) -> np.ndarray:
    '''
    Creates a boolean mask to select all rows of df in which the column
    key has a particular value or is in a set of values.

    Parameters
    ----------
    df: pandas.DataFrame
        The dataframe to filter
    column:
        A df column of sequence of length one with a column
    values:
        An iterable of the values or a single value that each row must match
        to be selected
    exclude: bool
        Whether to include or exclude the passed values. If True, all rows
        which have values different from those in the passed values will
        be selected. Otherwise, those rows which have values in those passed
        will be selected.

    Returns
    -------
    mask: numpy.ndarray
        a boolean array that can be used to index df to select the matching
        rows
    '''
    if is_iterable(values):
        mask = df[column].isin(values)
    else:
        mask = df[column] == values
    return (~mask if exclude else mask).values.ravel()


def mask_tuple(df: pd.DataFrame, columns: Sequence[Any],
               values: Sequence[Any],
               exclude: bool = False) -> np.ndarray:
    '''
    Creates a boolean mask to select all rows of df in which the passed columns
    each match the values passed.

    This function creates a mask that selects in an all or nothing fashion:
    either all columns in row match the corresponding values or it will not
    be selected. To emulate the behaviour of matching rows who match the value
    in any of the columns, one would have to use filter_dataframe using OR
    as connective.

    Parameters
    ----------
    df: pandas.DataFrame
        The dataframe to filter
    column:
        A sequence of columns in df
    values:
        Must have the same length as the passed columns. Each element
        corresponds to a column and is the value to be matched.
    exclude: bool
        If True, select all rows who have, for each passed column the exact
        corresponding value passed. If False, select all rows except those.

    Returns
    -------
    mask: numpy.ndarray
        a boolean array that can be used to index df to select the matching
        rows
    '''
    mask = reduce(
            operator.and_,
            itertools.starmap(
                    lambda k, v: operator.eq(df.loc[:, k], v),
                    zip(columns, values)
            ),
            np.array([True], dtype=np.bool)
    )
    return (~mask if exclude else mask).values.ravel()


def mask_tuples(df, columns, values, exclude=False):
    '''
    Creates a boolean mask to select all rows of df in which the passed columns
    each match exactly any of the tuples of values passed.

    This function creates a mask that selects in an all or nothing fashion:
    either all columns in row match the corresponding values or it will not
    be selected. To emulate the behaviour of matching rows who match the value
    in any of the columns, one would have to use filter_dataframe using OR
    as connective.

    Parameters
    ----------
    df: pandas.DataFrame
        The dataframe to filter
    column:
        A sequence of columns in df
    values:
        Must have the same length as the passed columns. Each element
        corresponds to a column and is the value to be matched.
    exclude: bool
        If True, select all rows who have, for each passed column the exact
        corresponding value passed. If False, select all rows except those.

    Returns
    -------
    mask: numpy.ndarray
        a boolean array that can be used to index df to select the matching
        rows
    '''
    if exclude:
        reduce_func = operator.and_
        base_val = np.array([True], dtype=np.bool)
    else:
        reduce_func = operator.or_
        base_val = np.array([False], dtype=np.bool)
    return reduce(
            reduce_func,
            map(lambda x: mask_tuple(df, columns, x, exclude), values),
            base_val
    )


def mask_tuples_iter(df, columns, values, exclude=False):
    if not all(map(lambda x: isinstance(x, tuple), values)):
        values = [tuple(val) for val in values]
    if exclude:
        func = lambda x: x not in values
    else:
        func = lambda x: x in values
    return np.fromiter(
            map(func, df.loc[:, columns].itertuples(index=False, name=None)),
            dtype=np.bool
    )


def construct_mask(df: pd.DataFrame, columns: Union[Any, Iterable[Any]],
                   values: Union[Any, Iterable[Any], Iterable[Iterable[Any]]],
                   exclude: bool = False):
    '''
    Constructs an appropriate boolean mask depending on the format of the
    passed columns and values.

    If columns is an iterable and values is an iterable whose elements are also
    iterables with the same length as columns, then mask_tuples is called,
    otherwise, mask_single is called.

    Parameters
    ----------
    df: pd.DataFrame
        The dataframe to filter
    columns:
        The columns to be filtered by
    values:
        The values of the columns to filter
    exclude:
        whether the values passed are to be included or excluded from the mask
    '''
    columns = list(columns) if is_iterable(columns) else columns
    values = list(values) if is_iterable(values) else values
    if (is_iterable(columns) and is_iterable(values)
            and is_iterable(values[0])
            and len(columns) == len(values[0])):
        # return mask_tuples_iter(df, columns, values, exclude)
        return mask_tuples(df, columns, values, exclude)
    return mask_single(df, columns, values, exclude)


def construct_boolean_mask(
        df: pd.DataFrame,
        to_include: Optional[
            Mapping[Union[Any, Iterable[Any]],
                    Union[Any, Iterable[Any], Iterable[Iterable[Any]]]
            ]
        ] = None,
        to_exclude: Optional[
            Mapping[Union[Any, Iterable[Any]],
                    Union[Any, Iterable[Any], Iterable[Iterable[Any]]]
            ]
        ] = None,
        merge: str = "and"):
    '''
    Function that calls construct_mask for each of the conditions in
    to_exclude or to_include.

    Parameters
    ----------
    df: pandas.DataFrame
        Dataframe to filter
    to_include:
        A mapping with the format specified in filter_dataframe. Rows
        that satisfy the condition will be included
    to_exclude:
        A mapping with the format specified in filter_dataframe. Rows
        that satisfy the condition will not be included
    merge:
        How to combine the masks that result from each condition. Accepted
        values are "and", "or".
    '''
    if to_include:
        exclude = False
        filter_values = to_include
    elif to_exclude:
        exclude = True
        filter_values = to_exclude
    else:
        raise ValueError("Must pass either to_include or to_exclude")

    if merge == 'and':
        merge_func = operator.and_
        default = np.array([True], dtype=np.bool)
    elif merge == "or":
        merge_func = operator.or_
        default = np.array([False], dtype=np.bool)
    else:
        raise ValueError("merge must be in ['and', 'or']")
    return reduce(
            merge_func,
            itertools.starmap(
                    lambda k, v: construct_mask(df, k, v, exclude),
                    filter_values.items()
            ), default
    )


def filter_dataframe(df, to_include=None, to_exclude=None, connective='and'):
    '''
    Only one the arguments can be passed each call. If to_include is passed,
    the query will select rows that contain the specified values. If to_exclude
    is passed then the query will select rows which do not contain the values.

    A dictionary must be passed. The key format must be one of:
        - A dataframe column name. In this case, the value must be a list-like
        that contains the values of the column to filter by or a single value.
        - A list-like. In these case, each of the elements must be a dataframe
        column name. The value must be a list-like, whose elements are also
        list-likes and a size equal to the number of elements in the key. The
        latter contain the values corresponding to each column in the key.
        This is useful to filter rows that must have certain values in several
        columns.

    Arguments
    ---------------
    to_include: dict
        Contains the values to filter by. Must comply with the format.
    to_exclude: dict
        Contains the values to filter by. Must comply with the format.
    '''
    if to_include and to_exclude:
        raise ValueError("Cannot include and exclude simultaneously")
    if not (to_include or to_exclude):
        return df

    mask = construct_boolean_mask(df, to_include, to_exclude, connective)
    return df.loc[mask]


def compute_common_values(df, by, redundant_col, redundant_values):
    values_other_cols = []
    other_cols = set(by) - {redundant_col}
    df = filter_dataframe(
            df, to_include={redundant_col: redundant_values}).sort_values(by)
    for _, group in df.groupby(redundant_col):
        values_other_cols.append(
                set(group[other_cols].drop_duplicates(other_cols).itertuples(
                        index=False, name=None)
                )
        )
    lists = list(map(list, reduce(lambda x, y: x & y, values_other_cols)))
    index = by.index(redundant_col)
    base = []
    # se insertan los valores correspondientes en las posiciones de by
    for val in redundant_values:
        for l in lists:
            cop = l.copy()
            cop[index: index + 1] = [val] + cop[index: index + 1]
            base.append(cop)
    return base


def compute_common_tuples(df, by, redundant_col, redundant_values):
    index = by.index(redundant_col)
    tups = compute_common_values(df, by, redundant_col, redundant_values)
    return zip(*map(
            lambda x: list(x[-1]),
            itertools.groupby(tups, key=lambda x: x[index]))
    )


def compute_common_tuples_equal_field(df, by, redundant_col, redundant_values):
    tups = filter_dataframe(
            df, to_include={redundant_col: redundant_values})[
               by].drop_duplicates().sort_values(by).sort_values(redundant_col)
    return zip(*map(
            lambda x: list(x[-1]),
            itertools.groupby(tups.itertuples(index=False),
                              key=lambda x: getattr(x, redundant_col))
            )
    )

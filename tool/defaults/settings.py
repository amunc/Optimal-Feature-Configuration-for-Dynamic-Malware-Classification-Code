# -*- coding: utf-8 -*-
"""
Created on Tue Jul  7 08:49:10 2020

@author: David
"""
import os
# Switches to deactivate certain steps
# When invoking from command line, only the ones specified will be enabled
FEATURE_EXTRACTION_ENABLED = True
MODEL_GENERATION_ENABLED = True
STATISTICS_ENABLED = True

# Path which contains the directory with data files to extract features from
BASE_DATA_PATH = r"/media/alumn0/Elements SE/reports/global"


# To load classes for the extraction scheme that are in other directories,
# those directories must be added to python path for module search
ADD_TO_PYTHON_PATH = []

# Delete the files contained in the specified output directories, which are
# datasets, info and statistics by default for the feature_extraction,
# model_generation and statistics modules respectively
DELETE_FILES_AT_START = False

ERROR_LOG_NAME = "error_log.txt"
ERROR_LOG_PATH = os.path.join(os.path.dirname(os.path.dirname(__file__)), ERROR_LOG_NAME)


FULL_LOG_NAME = "full_log.txt"
FULL_LOG_PATH = os.path.join(os.path.dirname(os.path.dirname(__file__)), FULL_LOG_NAME)


# Configuration for logging
# By default prints all messages to stdin and FULL_LOG_NAME and errors and
# above to ERROR_LOG_NAME
# "file_handler" handler is defined because autosklearn innner workings
LOG_CONFIG = {
    "version": 1,
    "disable_existing_loggers" : False,
    "formatters": {
        "full_formatter": {
            "format": "[%(asctime)s]: %(name)s %(levelname)s %(message)s",
            "datefmt": "%d-%m-%Y %H:%M:%S"
        }
    },
    "filters": {
        "own_only": {
            "()": "tool.utils.logging.FilterByName",
            "name": os.path.basename(os.path.dirname(__file__))
        }
    },
    "handlers": {
        "console": {
            "class": "logging.StreamHandler",
            "level": "DEBUG",
            "formatter": "full_formatter",
            "stream": "ext://sys.stdout"
        },
        "error": {
            "class": "logging.FileHandler",
            "filename": ERROR_LOG_PATH,
            "mode": "w",
            "level": "ERROR",
            "formatter": "full_formatter"
        },
        "full": {
            "class": "logging.FileHandler",
            "filename": FULL_LOG_PATH,
            "mode": "w",
            "level": "INFO",
            "formatter": "full_formatter",
            "filters": ["own_only"]
        },
        "file_handler": {
            "class": "logging.FileHandler",
            "level": "DEBUG",
            "formatter": "full_formatter",
            "filename": "autosklearn.log"
        }
    },
    "root": {
        "level": "DEBUG",
        "handlers": ["console", "error", "full", "file_handler"]
    }
}

FEATURE_DIR = "datasets"

# The output directory for the feature extraction phase
FEATURE_OUTPUT_PATH = os.path.join(os.path.dirname(os.path.dirname(__file__)), FEATURE_DIR)

# Output files for default feature extracion, defined here for convenience
API_FEATURES_FILE_NAME = "api.csv"
SIGNATURES_FEATURES_FILE_NAME = "signatures.csv"
NETWORK_FEATURES_FILE_NAME = "network.csv"
LABELS_FILE_NAME = "labels.csv"

# Arguments to pass to the "process" method of the first classes defined in
# extraction scheme
INIT_ARGS = tuple()
INIT_KWARGS = {}

# Names defined here for convenience
LABEL_COLUMN = "Type"
IDENTIFIER_COLUMN = "Sha1"

# Defines the order of computations in the feature extraction process
# Each element must be a dictionary which the following keys:
# name: a class which must implement a "process" method
# args (optional): arguments for the class constructor
# kwargs (optional): keywork arguments for the class constructor
# next (optional): a list which contains other dictionaries in this format
#                  each of the elements will be passed the return value of the
#                  process method of the class as argument to their own process
#                  method

# NOTE: by default, all keyword arguments with name "file" in this scheme will
# be concatenated with FEATURE_OUTPUT_PATH if they are not a relative or
# absolute path
EXTRACTION_SCHEME = [
    {"name": "tool.feature_extraction.path_generators.subdirectories.SubdirectoryPathGenerator",
     "kwargs": {"base_dir": BASE_DATA_PATH},
     "next": [
            {
             "name": "tool.feature_extraction.extractors.id_from_path.IdFromPath",
             "next": [
                      {
                       "name": "tool.feature_extraction.extractors.label_from_file.LabelFromFile",
                       "kwargs": {"label_file": os.path.join(BASE_DATA_PATH, "labels.csv"),
                                  "label_column": LABEL_COLUMN,
                                  "id_column": IDENTIFIER_COLUMN},
                       "next": [
                                {
                                 "name": "tool.feature_extraction.writers.dict_to_dataframe.DictToDataFrameWriter",
                                 "kwargs": {"file": LABELS_FILE_NAME}
                                }
                            ]
                       }
                ]
            },
            {
             "name": "tool.feature_extraction.loaders.json_loader.JsonLoader",
             "kwargs": {},
             "next": [
                {
                 "name": "tool.feature_extraction.extractors.api.APIFeatureExtractor",
                 "next": [
                         {
                          "name": "tool.feature_extraction.writers.dict_to_dataframe.DictToDataFrameWriter",
                          "kwargs": {"file": API_FEATURES_FILE_NAME}
                          }
                     ]
                },
                {
                 "name": "tool.feature_extraction.extractors.signature.SignatureFeatureExtractor",
                 "next": [
                          {
                           "name": "tool.feature_extraction.writers.dict_to_dataframe.DictToDataFrameWriter",
                           "kwargs": {"file": SIGNATURES_FEATURES_FILE_NAME}
                          }
                         ]
                  }
             ]
             },
            # a Different loader
            {
             "name": "tool.feature_extraction.loaders.pcap_loader.PcapLoader",
             "next": [
                {
                 "name": "tool.feature_extraction.extractors.network.NetworkFeatureExtractor",
                 "next": [
                         {
                          "name": "tool.feature_extraction.writers.dict_to_dataframe.DictToDataFrameWriter",
                          "kwargs": {"file": NETWORK_FEATURES_FILE_NAME}
                          }
                     ]
                 }
              ]
            }
        ]
     }
]

# After extracting features, perhaps there is a need to perform some operations
# such as merging datasets. After finishing feature extraction the object
# specified her will be called by passing an instance of the settings first and
# then the arguments specified here
# Passing the settings offers the possibility of changing them during execution
# for example, to modify the output files to be used for model generation
FEATURE_POSTPROCESSING = {
        "name": "tool.feature_extraction.postprocessing.combine.combine",
        "args": tuple(),
        "kwargs": {"to_combine_later": [LABELS_FILE_NAME]}
}


# Here go the paths to the files to be loaded by the model generation module
# if they are not absolute or relative paths, they will be prefixed
# FEATURE_OUTPUT_PATH, understanding that they are located there
MODEL_GENERATION_FILES_TO_LOAD = [
    "api_network.csv", "api_signatures.csv", "network_signatures.csv",
    "api_network_signatures.csv"
]


# Arguments defined here for convenience
TARGET_COL = LABEL_COLUMN
COLUMNS_TO_DISCARD = [IDENTIFIER_COLUMN]


# Loader for the model generation phase. Must define a load method with a
# single argument, which is a path with the file to be loaded
# The load method must return a 2-tuple of numpy arrays: first the features
# in a 2-dimensional array (n, d) and labels in a 1-dimensional array (n, )
# args and kwargs are optional and will be passed to the constructor
MODEL_GENERATION_DATA_LOADER = {
        "name": "tool.model_generation.data_loaders.csv_loader.CSVSplitLoader",
        "args": tuple(),
        "kwargs": {"target_column": TARGET_COL, "columns_to_discard": COLUMNS_TO_DISCARD}
}

# Number of trials. For each dataset, an autosklearn model will be generated
# for each HPO selected.
NUM_OF_TRIALS = 1

# Configuration of log for autosklearn. This is specified to stop autosklearn
# from overriding out log configuration
AUTOSKLEARN_LOG_CONFIG = {
    "version": 1,
    "disable_existing_loggers" : False,
    "formatters": {
        "full_formatter": {
            "format": "[%(asctime)s]: %(name)s %(levelname)s %(message)s",
            "datefmt": "%d-%m-%Y %H:%M:%S"
        }
    },
    "handlers": {
        "file_handler": {
            "class": "logging.FileHandler",
            "level": "DEBUG",
            "formatter": "full_formatter",
            "filename": "autosklearn.log"
        }
    },
    "incremental": True
}


# Arguments to be passed to the AutosklearnClassifier constructor
# All parameters at: https://automl.github.io/auto-sklearn/master/api.html
AUTOSKLEARN_PARAMS = {
        "time_left_for_this_task": 60 * 10,
        "per_run_time_limit": 60 * 10,
        "resampling_strategy": "cv",
        "resampling_strategy_arguments": {"folds": 10},
        "ml_memory_limit": 2 ** 10 * 8,  #Must be int
        "ensemble_memory_limit": 2 ** 10 * 2, #Must be int
        "logging_config": AUTOSKLEARN_LOG_CONFIG
}

# The HPO algorithms to be applied on each dataset
# Available values are "SMAC", "ROAR" and "RandomSearch"
AUTOSKLEARN_HPO = ["SMAC", "ROAR", "RandomSearch"]

# This allows to specify different parameters for each dataset. For example,
# one dataset may be bigger than others and thus require more training time
# this may be arranged by setting the appropriate value in "classifier_params"
# which will override AUTOSKLEARN_PARAMS for the specified dataset
# the key must be the file name of the dataset, without file extension
PER_DATASET_PARAMS = {
    "api": {
        "fit_params": {
            "dataset_name": "api",
        },
        "classifier_params": {
            "time_left_for_this_task": 60 * 10
        }
    }
}

# Output path for the files in model generation phase
INFO_OUTPUT_PATH = os.path.join(os.path.dirname(os.path.dirname(__file__)), "info")
INFO_OUTPUT_FILE = os.path.join(INFO_OUTPUT_PATH, "info.csv")
INFO_FILE_PATH = INFO_OUTPUT_FILE

# Defined here for convenience
METRICS_DICT = {
        "Accuracy": "sklearn.metrics.accuracy_score",
        "Mcc": "sklearn.metrics.matthews_corrcoef",
        "Sensitivity": "tool.model_generation.info_extraction.basic.sensitivity_score",
        "Specificity": "tool.model_generation.info_extraction.basic.specifity_score"
}


# After fitting the model, some information or metrics may be extracted
# All callables defined here must return a dictionary. All returned
# dictionaries will be merged and will be stored in a csv. This means that if
# the keys overlap, they will be overrridden, so beware of that.
MODEL_INFO_EXTRACTION = {
    # These return a dictionary with predefined keys. In case you want to
    # put all the processing in a single function. Each element must be a path
    # to a callable
    # Each will receive the AutoSklearnClassifier fitted, the features and
    # its corresponding labels. These functions must return a dictionary
    "extractors": [],
    # These receive as argument the generated model, features and corresponding
    #labels
    "general": {
            "Dataset": "tool.model_generation.info_extraction.basic.get_dataset",
            "Best-Model": "tool.model_generation.info_extraction.basic.get_best_model_name",
            # "Hyperparameters": "tool.model_generation.info_extraction.basic.get_sorted_params",
            "Selection_alg": "tool.model_generation.info_extraction.basic.get_hpo"
    },
    # These ones are computed by cross-validation taking a mean
    "prediction": [
        {
                # Since autosklearn returns an ensemble, pass a function
                # that extracts the desired model from it
                "estimator": "tool.model_generation.info_extraction.basic.construct_best_model",
                # Since the same metrics may be extracted for different
                # estimators, set a prefix for the dictionary keys to
                # distinguish them
                "prefix": "Best_model_",
                # The format for this dictionary must have strings as keys and
                # callable funcstions as values. These functions will receive
                # two arguments: the predicted labels and the true labels
                # The functions must return a single scalar value representing
                # the metric
                "metrics": METRICS_DICT,
                "computation": "tool.model_generation.info_extraction.basic.get_cross_validation_metrics"
        },
        {
                "estimator": "tool.model_generation.info_extraction.basic.construct_ensemble",
                "prefix": "Ensemble_",
                "metrics": METRICS_DICT,
                "computation": "tool.model_generation.info_extraction.basic.get_cross_validation_metrics"
        }
    ]
}


# Analogous to FEATURE_POSTPROCESSING
MODEL_POSTPROCESSING = {
    "name": "tool.model_generation.postprocessing.postprocessing.set_default_columns",
    "args": tuple(),
    "kwargs": {"data_file": INFO_OUTPUT_PATH}
}


# The p-value that signals statistical significance
BASE_PVALUE = 0.05

# Path to the file to be loaded for the computations
STATISTICS_INPUT_FILE = INFO_OUTPUT_FILE

# Columns but the values to be statistically compared. These will usually be
# metrics computed from the generated models
STATISTICS_DATA_COLUMNS = ["Best_model_Accuracy", "Best_model_Mcc",
                           "Ensemble_Accuracy"]
STATISTICS_DATA_COLUMNS = ["Accuracy", "Mcc",
                           "Ensemble_Accuracy"]

# Columns which contains the treatments to be compared by their data_columns
# Basically, we will rank them on each of the data columns to determine which
# is the best
STATISTICS_DESCRIPTIVE_COLUMNS = ["Dataset", "Selection_alg"]

REDUNDANT_INFORMATION = [
    {"Dataset": ("api_network_signatures", "api_categorical_network_signatures")},
    {"Dataset": ("api_network", "api_categorical_network")},
    {"Dataset": ("api_signatures", "api_categorical_signatures")}
]

# Perhaps some of the treatments on the data contain the same information and
# we want to keep only the best. Each element of the list must be a dictionary
# specifying a single descriptive column and the values that are redundant
REDUNDANT_INFORMATION = [
    {"Dataset": ("apis_network_firmas", "apis_categorical_network_firmas")},
    {"Dataset": ("apis_network", "apis_categorical_network")},
    {"Dataset": ("apis_firmas", "apis_categorical_firmas")}
]

# When removing redundant information, this function will be called
# and must return a list with the redundant values to be eliminated of an empty
# list if none will be deleted
# Receives as argument a single dictionary with two keys:
# "ranking", which contains a list of DescriptiveStats sorted from best to worst
# "comparisons", a dictionary whose keys a 2-tuple with the values of the
#                descriptive columns being compared and have for values
#                a 2-tuple with the pvalue of the comparison and a Order
#                instance which determines if the first tuple is better
#                (Order.GREATER), worse (Order.LOWER) or there are no
#                significant differences (Order.EQUAL)
REDUNDANT_INFORMATION_REMOVAL_FUNCTION = "tool.statistics.redundancy.functions.select_best"


# Must return a pandas.DataFrame from the path passed to its load method
STATISTICS_DATA_LOADER = {
        "name": "tool.model_generation.data_loaders.csv_loader.CSVLoader",
        "args": tuple(),
        "kwargs": {}
}

# Compute the comparison by first eliminating the worst values from the
# descriptive columns and finally comparing the beast of each
STATISTICS_BY_ELIMINATION = True

# Order of which columns will have its worse values eliminated first
COLUMN_EVALUATION_ORDER = ["Selection_alg", "Dataset"]

# If the statistical tests signal that there are significant differences
# the value returned by this function will be used to determine whic is best
# It will be passed a pandas.Series containing the values and must return
# a comparable object
DIFFERENCE_FUNCTION = "numpy.median"

# Statistical tests to be applied
# Test that checks whether a certain set of values follow a normal distribution
# receives a single pandas.Series with the observations and must return a
# pvalue. If this pvalue is greater than BASE_PVALUE, the data will be
# considered to be normal
NORMALITY_TEST = "tool.statistics.tests.default.normality_test"

# test that checks whether there are statistically signifcant differences
# between a set of observations. Receives a variable number of pandas.Series
# and returns a pvalue. If greater than BASE_PVALUE, post-hoc tests will be
# applied
DIFFERENCE_TEST = "tool.statistics.tests.default.friedman"

# tests that checks whether there are differences between two sets of
# observations. Receives two pandas.Series and returns a pcalue. If the pvalue
# is lower than the corrected (with Bonferroni) BASE_PVALUE, there are
# differences and which is the best will be checked with the value returned by
# DIFFERENCE_FUNCTION for each observation set
# POSTHOC_NORMAL_TEST will be applied when both observation sets are normal,
# otherwise, POSTHOC_TEST will be applied
POSTHOC_NORMAL_TEST = "tool.statistics.tests.default.t_student"
POSTHOC_TEST = "tool.statistics.tests.default.wilcoxon"


# saves in JSON format the rankings and comparisons made during the elimination
# process if STATISTICS_BY_ELIMINATION and also in redundancy elimination
REGISTER_INTERMEDIATE_RESULTS = True

# Path with the output of the files of the module
STATISTICS_OUTPUT_PATH  = os.path.join(os.path.dirname(os.path.dirname(__file__)), "statistics")
STATISTICS_OUTPUT_FILE  = os.path.join(STATISTICS_OUTPUT_PATH, "statistics.json")

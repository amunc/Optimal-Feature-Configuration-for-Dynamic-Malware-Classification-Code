#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Fri Jul 10 13:14:54 2020

@author: alumno
"""
import os
from ...feature_extraction.features.utils.dictionary import prefix_dict_keys
import sklearn.metrics
from sklearn.metrics import accuracy_score, matthews_corrcoef
from sklearn.metrics import make_scorer
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import multilabel_confusion_matrix
from sklearn.model_selection import cross_validate
from autosklearn.pipeline.classification import SimpleClassificationPipeline
from autosklearn.classification import AutoSklearnClassifier
import numpy as np
from ..autosklearn_hpo import DICT_HPO_ALG
import copy


def get_best_model(auto):
    return get_sorted_params(auto)[0][0]


def get_ensemble_params(auto):
    return [model.config.get_dictionary()
            for _, model in auto.get_models_with_weights()]


def get_sorted_params(auto):
    params = auto.cv_results_['params']
    scores = auto.cv_results_['mean_test_score']
    # this is irrelevant since all models in the ensemble must be successful
    # status = auto.cv_results_['status']

    # Extract the configuration dictionary for each model
    ensemble_params = get_ensemble_params(auto)
    # get the index of each model parameters in the cv_results_ array
    indices = [params.index(model_params) for model_params in ensemble_params]
    # get the parameters, score and status
    actual_params = [(params[ind], scores[ind]) for ind in indices]
    # sort by score
    params_sorted = sorted(actual_params, reverse=True, key=lambda tup: tup[1])
    return params_sorted

def get_best_model_name(auto):
    return get_best_model(auto)["classifier:__choice__"]


def get_hpo(auto):
    return {value: key for key, value
            in DICT_HPO_ALG.items()}[auto.get_smac_object_callback]


def get_dataset(auto):
    return os.path.basename(auto.filename).rsplit(".", 1)[0]


def construct_best_model(auto):
    return SimpleClassificationPipeline(get_best_model(auto))


def construct_ensemble(auto):
    # cross_validate clones the passed estimator and that is bad
    # since AutoSklearnClassifier takes a while to fit, and is fitted already
    # so this little hack bypasses the cloning and makes it possible to
    # use it as a run-of-the-mill estimator
    class MyAutoSklearnClassifier(AutoSklearnClassifier):
        def __init__(self, **kwargs):
            self.auto = copy.deepcopy(auto)
        def __getattr__(self, name):
            return getattr(self.auto, name)
        def fit(self, X, y, **fit_params):
            return self.refit(X, y)
    return MyAutoSklearnClassifier()


def get_matrix(true, pred):
    if np.unique(true).size == 2:
        return sklearn.metrics.confusion_matrix(true, pred)[np.newaxis, :, :]
    return multilabel_confusion_matrix(true, pred)

def sensitivity_score(true, pred):
    mcm = get_matrix(true, pred)
    tp = mcm[:, 1, 1]
    return (tp / (tp + mcm[:, 1, 0])).mean()


def specifity_score(true, pred):
    mcm = get_matrix(true, pred)
    tn = mcm[:, 0, 0]
    return (tn / (tn + mcm[:, 0, 1])).mean()


DEFAULT_METRICS = {"Accuracy": accuracy_score, "Mcc": matthews_corrcoef,
                   "Sensitivity": sensitivity_score,
                   "Specificity": specifity_score}


def get_default_characteristics(auto, X, y):
    metrics = {}
    best_model = get_best_model(auto)
    metrics["Dataset"] = get_dataset(auto)
    metrics["Best-Model"] = get_best_model_name(auto)
    metrics["Hyperparameters"] = get_sorted_params(auto)
    metrics["Selection_alg"] = get_hpo(auto)
    best_model = SimpleClassificationPipeline(best_model)
    if auto.ensemble_size != 0:
        metrics.update(
            prefix_dict_keys("Ensemble",
                    get_cross_validation_metrics(auto, X, y, DEFAULT_METRICS)
            )
        )
    metrics.update(
        prefix_dict_keys("Best_model",
                get_cross_validation_metrics(best_model, X, y, DEFAULT_METRICS)
        )
    )
    return metrics


def get_cross_validation_metrics(estimator, X, y, metrics):
    metric_scorers = {}
    for metric, func in metrics.items():
        metric_scorers[metric] = make_scorer(func)
    results = {}
    cv = StratifiedKFold(n_splits=10)
    metric_values = cross_validate(estimator, X, y, scoring=metric_scorers,
                                   cv=cv)
    for metric in metrics:
        results[metric] = metric_values["test_" + metric].mean()
    return results
